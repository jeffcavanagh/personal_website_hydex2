---
title: "Processing Text Data"
author: "Jeff Cavanagh"
date: '2020-08-05'
lastmod: '2020-08-05T15:49:17-04:00'
layout: post
highlight: yes
slug: processing-text-data
tags:
- stringr
- tidytext
categories: 
  - R
type: post
output:
  blogdown::html_page:
    toc: true
    toc_depth: 4
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---

**Text data cannot be processed identically to numerical or categorical data. Is there a way then to analyze text data and obtain insight from it??**

Of course! All it takes is an adapted approach and a little more work upfront.

Quite a bit of potential data is unstructured and textual in nature. To extract the underlying value of this category of data, different techniques are required.

In this post we will focus on some routine methods for text mining and text analysis in R and how they can be utilized to process and draw value from unstructured text data. 

### Setup 

In R, the first stop is the `stringr` package. This package can be loaded on its own, or as part of `tidyverse`.

The functions contained within process character strings in various ways. Luckily for us, R has a [cheatsheet](/rcheatsheets/strings.pdf){target="blank"} for the package.

It should be noted that there is also the `stringi` package which contains more robust functions for handling strings. For simplicity, we will only be concentrating on `stringr` in this post.

```{r, warning = F, message = F}
library(stringr)
library(tidyverse)
```

To illustrate the use of text manipulation in R, we will be analyzing a pdf of Mary Shelley's [*Frankenstein*](/analytics/2020-08-05-processing-text-data_files/frankenstein.pdf){target="blank"} (downloaded from [planetbook.com](https://www.planetebook.com/free-ebooks/frankenstein.pdf){target="blank"}). We will use the `pdftools` package to extract the text from the pdf file. 

```{r, warning = F, message = F}
library(pdftools)
book <- pdf_text("frankenstein.pdf")
```

`pdf_text` converts the pdf file into a character string where each element is a page from the document.

Now that we have defined the book as a character string we will begin to mold it into a data frame so it is easier to work with.

```{r}
book_df <- book  %>% 
  as_tibble_col("content") %>%
  rowid_to_column("page_number")

book_df
```

This gives a data frame where each row is an observation containing the page number and content held on that page. Now we can begin in earnest with the text mining. 

* `str_detect` is used to detect which observations match a pattern
* `str_which` is used to pick out the index's of the entries of all matches
* `str_extract` is used to pull out the first pattern match within each element
  + Some functions in `stringr` also have a `_all` functionality, such as `str_extract_all`, which returns all matches within an observation
  
Here is a look at these three function in practice. To illustrate the use of these functions, we will search for all pages that reference "Frankenstein".

```{r}
book_df$content %>%
  str_detect("Frankenstein") %>%
  head()

book_df$content %>%
  str_detect("Frankenstein") %>%
  sum()

book_df$content %>%
  str_detect("Frankenstein") %>%
  mean()

```

Adding on the `sum` and `mean` functions we are also able to see the number and percentage of pages which contain the "Frankenstein" text pattern.

Next, `str_which` gives us the locations of the pages containing the pattern.
```{r}
ind_capitals <- book_df$content %>%
  str_which("Frankenstein")

ind_capitals
```

Lastly, `str_extract` pulls the pattern from each of the pages. However, `str_extract` will return a vector as the same length as the input. In order to filter to only entries that have the pattern match, we will combine it with `str_which`.

```{r}
book_df$content[ind_capitals] %>%
  str_extract("Frankenstein")
```

Some other characters to take note of: `^` is used to search for matches at the start of a string, `$` is used to search for matches at the end of a string, `.` is used to match any character that is not a new line, `[:upper:]` is used to match upper case words, `*` is used to pull 0 or more matches, and `+` is used to pull 1 or more matches.

### Transforming Raw Text into Data Frame

The text data is now in a data frame, but is still far from orderly. Perusing through a few pages reveals that the footer at the bottom of each page is either the page number (which we already have) and the title of the book (Frankenstein), or the page number and the website home of the pdf. 

Neither of these are useful to us. Let's use `str_remove` to remove these from the pages.

```{r}
book_df$content <- book_df$content %>%
  str_remove("\\n(\\d+|\\030)\\s+Frankenstein\n$|\\n\\s*Free eBooks at Planet eBook.com\\s+(\\d+|\\\030)\\n$")
```

* `\\n` is used to detect new lines
* `\\d` is used to detect digits
* `\\s` is used to detect and spaces
* `|` is used as an or statement to detect multiple patterns

Now our strings contain only the most relevant information, but they are still dense and difficult to decipher insight from. 

The next step is to use the `tidytext` package to further break up the data. Using the `unnest_token` function it is possible to breakdown character strings into lines, paragraphs, sentences, words, and n-grams. First we will update the data frame by breaking the strings down by line.

```{r, warning = F}
library(tidytext)

book_df <- book_df %>%
  unnest_tokens("lines", content, token = "lines", to_lower = F)

book_df
```

Now that we have the text broken up into lines it is much easier to process. Next let's add more identifying variables where each line can be identified by line number and the chapter.

*Frankenstein* begins with several letters before it gets into the chapters. To identify which pages fall under letters or chapters we need to do a little preliminary work with the tools demonstrated so far.

```{r, warning = F, message = F}
ind_chapter <- book_df$lines %>%
  str_which("^Chapter") %>%
  append(nrow(book_df))

chapters <- book_df$lines[ind_chapter[-length(ind_chapter)]] %>%
  str_extract("^Chapter.*") %>%
  unlist()

ind_letter <- book_df$lines %>%
  str_which("^Letter") %>%
  append(ind_chapter[1])

letters <- book_df$lines[ind_letter[-length(ind_letter)]] %>%
  str_extract_all("^Letter.*") %>%
  unlist()
```

Now that we have our indexes and values we can create new columns to further classify each observation.

```{r}
book_df <- book_df %>%
  mutate(
    line_number = row_number(),
    chapter = "fill in"
  )
book_df$chapter[c(1:(ind_letter[1] - 1))] <- "Preface"

for (i in 1:(length(ind_letter) - 1)){
  book_df$chapter[c(ind_letter[i]:(ind_letter[i + 1] -1))] <- letters[i]
}

for (i in 1:(length(ind_chapter) - 1)){
  book_df$chapter[c(ind_chapter[i]:(ind_chapter[i + 1] -1))] <- chapters[i]
}

book_df <- book_df %>%
  select(chapter, page_number, line_number, lines)
```

`mutate` was used to add the new columns to the data frame and then `for` loops were used to set the values of the *chapter* column using the indexes and values that were established.

```{r}
book_df
```


What began as a pdf of unstructured text is now a structured data frame with clear variables and observations. Going forward this will be much easier to work with.

### Text Analysis

#### Counts

Now that we have a structured data frame, lets look for the most common words that appear within the text.

```{r, warning = F}
book_words <- book_df %>%
  unnest_tokens(word, lines, token = "words") 

book_words %>%
  count(word, sort = T)
```

The most common words are unsurprisingly the filler words found in any text. To sort these words out we will use the `stop_words` data frame, which is a list of these common words.

```{r, warnng = F}
book_words <- book_words %>%
  anti_join(stop_words) 

book_words %>%
  count(word, sort = T)
```

Now that we have this count, it would be nice to have a visualization of the most common words. Let's do that now using `ggplot`.

```{r}
book_words %>%
  count(word, sort = T) %>%
  filter(n > 60) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot() +
  geom_col(aes(n, word))
```

It is also useful to count combinations of words. `tidytext` gives this option using `ngrams`.

```{r}
book_df %>%
  unnest_tokens(bigram, lines, token = "ngrams", n = 2) %>%
  count(bigram, sort= T) # again the most common word pairings belong to the stop_words subset, we can still filter these out though

bigrams <- book_df %>%
  unnest_tokens(bigram, lines, token = "ngrams", n = 2)

bigrams_sep <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filter <- bigrams_sep %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigrams_count <- bigrams_filter %>%
  count(word1, word2, sort = T) %>% 
  na.omit()

bigrams_count
```

This same technique can be applied to any number of word groupings by setting the value of `n`.

We are also able to produce a nice visualization of these groupings using a word network.

```{r, warning = F}
library(grid)
library(igraph)
library(ggraph)

set.seed(1)
bigrams_count %>% 
  filter(n > 3) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(show.legend = F,
                 arrow = arrow(type = "closed", length = unit(0.1, "inches")),
                 aes(edge_alpha = n)) +
  geom_node_point(color = "green", size = 2) +
  geom_node_text(aes(label = name), repel = T) +
  theme_void()
```

In the above visualization arrows point from the first word to the second. The shading option (`edge_alpha`) included in the code makes an arrow darker the more times the word combination appears. We also added a filter that only allows word combinations which have more than 3 occurrences to appear in the network.

#### Document Frequencies

The last technique illustrated in this post concerns the frequency of text appearances within data. Word counts have there uses, but the number of appearances of a word or a grouping of words is not the best metric for how important those elements are to the overall data source.

*tf* is term frequency and represents how often a chosen token (word, bigram, trigram, sentence, etc) appears in a document.

*idf* is inverse document frequency and represents how unique a token is in a data set. It is calculated by the following formula:

$idf(\text{token}) = ln(\frac{n_{\text{documents}}}{n_{\text{documents containing token}}})$

Finally *tf-idf* is $tf * idf$. Below is an example of how to compute these values using the `bind_tf_idf` function.

```{r}
book_words %>%
  count(chapter, word, sort = T) %>%
  bind_tf_idf(word, chapter, n)
```

*tf-idf* is an important metric because it rates the importance of a token to a subsection of a dataset (in this case how significant each word is within a chapter), giving a higher value to a word the more used and more unique it is to its subsection. In our example we are only analyzing one book, but when working with a data source that contains multiple documents, the *tf-idf* statistic is extremely useful in identifying the most import words within each document that identify them most from the other documents.

When developing a machine learning model for text data, *tf*, *idf*, and *tf-idf* are a natural place to start as they give numerical values to categorical data which have significant implications on that data.


That concludes the post on text mining and processing in R. I first learned these methods from [Text Mining in R](https://www.tidytextmining.com/){target="blank"}, which is a phenomal resource for anyone learning how to work with text data.




