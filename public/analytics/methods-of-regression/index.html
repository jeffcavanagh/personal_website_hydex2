<!DOCTYPE html>
<html lang="en">
    
    


    <head>
    <link rel="icon" type="image/png" href="/img/favicon.ico">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.74.3" />

    
    
    

<title>Statistical Modeling Starter Kit • Jeff Cavanagh</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Statistical Modeling Starter Kit"/>
<meta name="twitter:description" content="This post will cover some of the primary methods of regression analysis available in R. It will also serve as a warmup for the posts that explore Machine Learning and its applications.
Setup For our regression data we will look at statistics concerning the history of my favorite sports team, The Detroit Lions. The data comes from Pro Football Reference.
lions &lt;- read.csv(&quot;sportsref_download - sportsref_download.csv&quot;) str(lions) ## &#39;data.frame&#39;: 90 obs."/>

<meta property="og:title" content="Statistical Modeling Starter Kit" />
<meta property="og:description" content="This post will cover some of the primary methods of regression analysis available in R. It will also serve as a warmup for the posts that explore Machine Learning and its applications.
Setup For our regression data we will look at statistics concerning the history of my favorite sports team, The Detroit Lions. The data comes from Pro Football Reference.
lions &lt;- read.csv(&quot;sportsref_download - sportsref_download.csv&quot;) str(lions) ## &#39;data.frame&#39;: 90 obs." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/analytics/methods-of-regression/" />
<meta property="article:published_time" content="2019-12-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-08-18T15:57:46-04:00" /><meta property="og:site_name" content="Jeff Cavanagh" />


    


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">








<link rel="stylesheet" href="/scss/hyde-hyde.16fcc2089a2ab10ac8df9820d5dee1901cd3c2ba5989e45dfaf9d546b7dc6536.css" integrity="sha256-FvzCCJoqsQrI35gg1d7hkBzTwrpZieRd&#43;vnVRrfcZTY=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">



    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->

    
    

</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title" style="font-size:1.85em">
        <a href="/">Jeff Cavanagh</a>
      </span>
      
        
        
        
        <div class="author-image">
          <img src="/img/coffeeandme_camping.JPG" alt="Author Image" class="img--round element--center">
        </div>
        
      
      
      <p class="site__description" style="font-style:italic;font-size:0.75em">
         OPTIMIZATION BY RECURSIVE ANALYSIS, MODELING, AND IMPLEMENTATION 
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Jeff Cavanagh</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="/about/">
						<span>About</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/projects/">
						<span>Projects</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/analytics/">
						<span>Analytics</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/fitness/">
						<span>Fitness</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/recipes/">
						<span>Recipes</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/perspective/">
						<span>Perspective</span>
					</a>
				</li>
			 
		
	</ul>
</div>

        <section class="social">
	
	
	
	<a href="https://github.com/jeffcavanagh" rel="me"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	<a href="https://linkedin.com/in/jeffcavanagh1/" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
	
	
	
	<a href="mailto:jmcavanagh15@gmail.com" rel="me"><i class="fas fa-at fa-lg" aria-hidden="true"></i></a>
	
</section>

      </div>
    </div>
    
<div class="copyright">
  &copy; 2019 - 2020 htr3n
  
</div>



  </div>
</div>

        <div class="content container">
            
    
<article>
  <header>
    <h1>Statistical Modeling Starter Kit</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Dec 12, 2019
    
    
    
      
      
          in
          
          
              <a class="badge badge-category" href="/categories/r">R</a>
              
          
      
    
    
    
      
      
          <br/>
           <i class="fas fa-tags"></i>
          
          <a class="badge badge-tag" href="/tags/ml">ml</a>
          
      
    
    
    <br/>
    <i class="fas fa-clock"></i> 27 min read
</div>


  </header>
  
  
  <div class="post">
    


<p><strong>This post will cover some of the primary methods of regression analysis available in R. It will also serve as a warmup for the posts that explore Machine Learning and its applications.</strong></p>
<div id="setup" class="section level3">
<h3>Setup</h3>
<p>For our regression data we will look at statistics concerning the history of my favorite sports team, The Detroit Lions. The data comes from <a href="https://www.pro-football-reference.com/teams/det/" target="&quot;_blank">Pro Football Reference</a>.</p>
<pre class="r"><code>lions &lt;- read.csv(&quot;sportsref_download - sportsref_download.csv&quot;)

str(lions)</code></pre>
<pre><code>## &#39;data.frame&#39;:    90 obs. of  29 variables:
##  $ Year       : int  2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 ...
##  $ Lg         : Factor w/ 1 level &quot;NFL&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Tm         : Factor w/ 3 levels &quot;Detroit Lions&quot;,..: 1 1 1 2 1 2 1 1 2 1 ...
##  $ W          : int  3 6 9 9 7 11 7 4 10 6 ...
##  $ L          : int  12 10 7 7 9 5 9 12 6 10 ...
##  $ T          : int  1 0 0 0 0 0 0 0 0 0 ...
##  $ Div..Finish: Factor w/ 19 levels &quot;1st of 4&quot;,&quot;1st of 5&quot;,..: 12 12 5 5 9 5 9 12 5 9 ...
##  $ Playoffs   : Factor w/ 6 levels &quot;&quot;,&quot;Lost Champ&quot;,..: 1 1 1 5 1 5 1 1 5 1 ...
##  $ PF         : int  341 324 410 346 358 321 395 372 474 362 ...
##  $ PA         : int  423 360 376 358 400 282 376 437 387 369 ...
##  $ PD         : int  -82 -36 34 -12 -42 39 19 -65 87 -7 ...
##  $ Coaches    : Factor w/ 27 levels &quot;Caldwell&quot;,&quot;Clark&quot;,..: 20 20 1 1 1 1 26 26 26 26 ...
##  $ AV         : Factor w/ 35 levels &quot;&quot;,&quot;Allen&quot;,&quot;Backus&quot;,..: 16 31 31 31 31 34 34 20 31 34 ...
##  $ Passer     : Factor w/ 40 levels &quot;&quot;,&quot;Batch&quot;,&quot;Clark&quot;,..: 37 37 37 37 37 37 37 37 37 11 ...
##  $ Rusher     : Factor w/ 41 levels &quot;&quot;,&quot;Abdullah&quot;,..: 18 18 2 28 2 3 6 21 4 4 ...
##  $ Receiver   : Factor w/ 47 levels &quot;&quot;,&quot;Barr&quot;,&quot;Box&quot;,..: 16 16 25 42 24 42 24 24 24 24 ...
##  $ Pts        : int  18 25 7 20 18 22 13 17 4 15 ...
##  $ Yds        : int  17 24 13 21 20 19 6 3 5 17 ...
##  $ Pts.1      : int  26 16 21 13 23 3 15 27 23 19 ...
##  $ Yds.1      : int  31 10 27 18 18 2 16 13 23 21 ...
##  $ T.G        : int  24 23 5 20 22 6 28 30 4 11 ...
##  $ Pts.       : int  24 21 13 21 18 13 13 23 8 17 ...
##  $ Yds.       : int  28 21 19 24 17 5 5 2 9 20 ...
##  $ out.of     : int  32 32 32 32 32 32 32 32 32 32 ...
##  $ MoV        : num  -5.1 -2.3 2.1 -0.8 -2.6 2.4 1.2 -4.1 5.4 -0.4 ...
##  $ SoS        : num  -0.1 -0.8 0.6 -0.6 2.4 -0.4 -2.8 1.8 0.6 2.3 ...
##  $ SRS        : num  -5.2 -3 2.7 -1.4 -0.2 2.1 -1.6 -2.3 6.1 1.9 ...
##  $ OSRS       : num  -1.2 -3.3 5.2 -1.3 1 -3.2 -1.2 2.1 6.9 2.4 ...
##  $ DSRS       : num  -4 0.3 -2.5 -0.1 -1.3 5.2 -0.5 -4.4 -0.8 -0.5 ...</code></pre>
<p>Before we continue with this data we should clean it up. Luckily all columns are either numeric, integer, or factor in format. This will make them easier to run the regression analysis on.</p>
<p>Some categories are missing section headers that needed to be taken out to read the data easily. Without context, some columns have the same names. We will go in add to these names so they are easier to understand.</p>
<p>Also, this analysis will only consider the Detroit Lions (even though the team was originally names the Portsmouth Spartans). Therefore we will remove the columns of the team’s name and league.</p>
<pre class="r"><code>names(lions)[c(4, 5, 6, 17, 18, 19, 20, 21, 22, 23, 24)] &lt;- c(
  &quot;wins&quot;, &quot;loss&quot;, &quot;ties&quot;, &quot;off.pts.rnk&quot;, &quot;off.yds.rnk&quot;, &quot;def.pts.rnk&quot;, 
  &quot;def.yds.rnk&quot;, &quot;tak.giv.rnk&quot;, &quot;pts.df.rnk&quot;, &quot;yds.df.rnk&quot;, &quot;num.teams&quot;
)

names(lions) &lt;- names(lions) %&gt;% str_to_lower()

lions &lt;- lions[, -c(2,3)]</code></pre>
<p>Now we have a clean data frame consisting of 27 variables and 90 rows (representing 90 years of play). We are able to now carry out our analysis.</p>
</div>
<div id="regression-models" class="section level3">
<h3>Regression Models</h3>
<p>Regression models divide datasets into predictor variables and response variables, and then use the predictor variables to create a model that forecasts the response variable.</p>
<div id="simple-linear-regression" class="section level4">
<h4>Simple Linear Regression</h4>
<p>First this data will be put through a simple linear regression model (regression with one predictor variable).</p>
<p><code>lm</code> is a built-in function of R that fits linear models. Applying it to the <code>lions</code> data set let ‘wins’ the response variable (the one we are attempting to predict) and <code>mov</code> (Margin og Victory) as the predictor variable (the one we will base the prediction on).</p>
<pre class="r"><code>slr_model &lt;- lm(wins ~ mov, data = lions)

summary(slr_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = wins ~ mov, data = lions)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.8672 -1.4241  0.0451  1.0530  4.6514 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  6.44107    0.16472   39.10   &lt;2e-16 ***
## mov          0.32411    0.02516   12.88   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.556 on 88 degrees of freedom
## Multiple R-squared:  0.6534, Adjusted R-squared:  0.6495 
## F-statistic: 165.9 on 1 and 88 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code>confint(slr_model)</code></pre>
<pre><code>##                 2.5 %    97.5 %
## (Intercept) 6.1137357 6.7684091
## mov         0.2741033 0.3741208</code></pre>
<p>The <code>summary</code> function gives an overview of the model (coefficients, residuals, levels of significance, etc.) and the <code>confint</code> function gives the confidence interval for each coefficent estimate (default is 95%).</p>
<p>To measure how significant each predictor variable is we will rely primarily on each individual p-value. To measure the accuracy of the model we will consider the <span class="math inline">\(R^2\)</span> and adjusted <span class="math inline">\(R^2\)</span> values, which will take on a value between 0 and 1 (the closer, the better the model), and the p-value of the entire model.</p>
<p>The output gives a simple linear regression model with the formula to predict the wins the Lions will have:</p>
<p><span class="math inline">\(\text{wins } = 6.44107 + 0.32411 * \text{ mov }\)</span></p>
<pre class="r"><code>plot(lions$mov, lions$wins)
abline(slr_model)</code></pre>
<p><img src="/analytics/2019-12-12-methods-of-regression_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Without going too in depth into the underlying statistical analysis, we can see from the p-values of each coefficient, the F-statistic, and the <span class="math inline">\(R^2\)</span> value that there is a significant relationship between <code>mov</code> and <code>wins</code>. This was expected as I showed that margin of victory is the most significant predictor for the number of wins a team will have in my <a href="/projects/nfl-team-wins-regression-analysis/index.html" target="_blank">2017 NFL Regression Project</a>.</p>
<p>However, this is a very simple model and there are many other variables at our disposal to increase its predictive capability. To take other variables under consideration, we will add them in using a multiple linear regression model.</p>
</div>
<div id="multiple-linear-regression" class="section level4">
<h4>Multiple Linear Regression</h4>
<p>To warm up for a including more variables, we will first only consider non-factor variables. We will also remove the year, loss, tie, and point differential (pd) variables.</p>
<pre class="r"><code>mlr_model &lt;- lm(wins ~ . - year - loss - ties - pd - div..finish - playoffs - coaches - av - passer - rusher - receiver, data = lions)

summary(mlr_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = wins ~ . - year - loss - ties - pd - div..finish - 
##     playoffs - coaches - av - passer - rusher - receiver, data = lions)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.8608 -0.5767 -0.0945  0.6123  3.2891 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.862894   0.638566   6.049 5.48e-08 ***
## pf           0.031060   0.023100   1.345   0.1829    
## pa          -0.026349   0.023052  -1.143   0.2567    
## off.pts.rnk -0.002395   0.095668  -0.025   0.9801    
## off.yds.rnk  0.025639   0.067506   0.380   0.7052    
## def.pts.rnk -0.018651   0.063583  -0.293   0.7701    
## def.yds.rnk  0.018234   0.054323   0.336   0.7381    
## tak.giv.rnk -0.059268   0.034163  -1.735   0.0869 .  
## pts.df.rnk  -0.030496   0.105752  -0.288   0.7739    
## yds.df.rnk   0.004136   0.078556   0.053   0.9582    
## num.teams    0.094658   0.059700   1.586   0.1171    
## mov         -1.297174   3.015030  -0.430   0.6683    
## sos         -1.243980   3.037990  -0.409   0.6834    
## srs         -0.268629   3.657786  -0.073   0.9417    
## osrs         1.441590   3.272233   0.441   0.6608    
## dsrs         1.553113   3.269455   0.475   0.6362    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.297 on 74 degrees of freedom
## Multiple R-squared:  0.7974, Adjusted R-squared:  0.7563 
## F-statistic: 19.41 on 15 and 74 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The <span class="math inline">\(R^2\)</span> value of the model went from 0.65 in the simple model to 0.79 in the multiple model, meaning noticeable improvement.</p>
<p>It is worth noting though that many p-values of the predictor variables are high, and therefore probably not significant. Next we will chip away at the model until it only considers predictor variables whose p-values are less than 0.05.</p>
<pre class="r"><code>mlr_model &lt;- lm(wins ~ pf + pa + tak.giv.rnk + num.teams, data = lions)

summary(mlr_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = wins ~ pf + pa + tak.giv.rnk + num.teams, data = lions)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.8256 -0.7525 -0.2356  0.6870  3.3084 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  4.366741   0.513026   8.512 5.26e-13 ***
## pf           0.024557   0.002367  10.375  &lt; 2e-16 ***
## pa          -0.021951   0.002451  -8.957 6.59e-14 ***
## tak.giv.rnk -0.085464   0.027127  -3.151  0.00225 ** 
## num.teams    0.109787   0.025739   4.265 5.15e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.254 on 85 degrees of freedom
## Multiple R-squared:  0.7824, Adjusted R-squared:  0.7721 
## F-statistic: 76.39 on 4 and 85 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>At the end of tinkering with which variables are significant predictors, we are left with <code>pf</code> (points for), <code>pa</code> (points against), <code>tak.giv.rank</code> (the ranking of a teams takeaway to giveaway ratio), and `num.teams (number of teams in the league that season). All of these variables have a p-value are well under 0.05, and while the <span class="math inline">\(R^2\)</span> value went down slightly, the adjusted <span class="math inline">\(R^2\)</span> value increased, telling us the model has improved.</p>
<p>We are also able to put in interactive terms and apply various evolutions to the predictor variables (logarithms, polynomial form, etc.). However, we will move on with the latest model and now include some of the qualitative predictor variables.</p>
<p>Some variables, (<code>div..finish</code> and <code>playoffs</code> mainly), are determined strongly by the amount of wins, and as such we will continue to exclude them from the model.</p>
<p>Instead we will focus on the components of the team: the coach, and leading player (<code>av</code>), <code>passer</code>, <code>rusher</code>, and <code>reciever</code>. These factors have many different levels (new players and coaches are constantly introduced over the 90 year history of the team). Therefore we wills start by adding a single variable <code>av</code>.</p>
<pre class="r"><code>mlr_model2 &lt;- update(mlr_model, ~ . + av)

summary(mlr_model2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = wins ~ pf + pa + tak.giv.rnk + num.teams + av, data = lions)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.9213 -0.5539  0.0000  0.2227  2.8098 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  5.593721   1.038731   5.385 1.85e-06 ***
## pf           0.023682   0.003061   7.738 3.74e-10 ***
## pa          -0.023667   0.002865  -8.259 5.73e-11 ***
## tak.giv.rnk -0.085190   0.035018  -2.433   0.0185 *  
## num.teams    0.035978   0.095422   0.377   0.7077    
## avAllen      1.236390   2.111245   0.586   0.5607    
## avBackus     2.477696   2.180637   1.136   0.2612    
## avBaker      1.142467   2.035565   0.561   0.5771    
## avBarney     0.269081   1.076318   0.250   0.8036    
## avBly        2.023394   2.337747   0.866   0.3908    
## avBoyd       2.882805   2.248760   1.282   0.2057    
## avBrown     -1.779144   1.353671  -1.314   0.1946    
## avCofer      0.448605   2.067743   0.217   0.8291    
## avCrockett   4.189631   2.024450   2.070   0.0436 *  
## avCrowell    1.821063   2.245398   0.811   0.4211    
## avDanielson  2.397690   2.064073   1.162   0.2508    
## avEnglish    0.391880   1.850832   0.212   0.8332    
## avFlanagan   0.062812   1.668687   0.038   0.9701    
## avFreitas    1.072930   1.877201   0.572   0.5701    
## avGolladay   0.235252   2.330507   0.101   0.9200    
## avGriffin    1.650084   2.045561   0.807   0.4236    
## avHipple     3.409371   2.033394   1.677   0.0997 .  
## avHunter     1.881138   2.095518   0.898   0.3736    
## avJohnson    1.121559   1.985821   0.565   0.5747    
## avKarras     0.892399   1.307842   0.682   0.4981    
## avKitna      2.469792   2.150261   1.149   0.2561    
## avLandry     0.442040   1.890402   0.234   0.8160    
## avLane       2.296162   1.329142   1.728   0.0901 .  
## avLary       1.802331   1.347065   1.338   0.1868    
## avMoore      1.569277   1.953902   0.803   0.4256    
## avMunson     1.394778   1.907161   0.731   0.4679    
## avRogers     1.659478   2.152440   0.771   0.4443    
## avSanders    2.664275   1.692314   1.574   0.1216    
## avSims       1.550223   1.811092   0.856   0.3960    
## avStafford   2.563475   1.995621   1.285   0.2048    
## avStewart    1.641590   2.340373   0.701   0.4862    
## avStudstill  1.598102   1.376949   1.161   0.2512    
## avSuh        2.125247   2.055131   1.034   0.3060    
## avWalker     1.165405   0.983964   1.184   0.2417    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.269 on 51 degrees of freedom
## Multiple R-squared:  0.8664, Adjusted R-squared:  0.7669 
## F-statistic: 8.704 on 38 and 51 DF,  p-value: 3.417e-12</code></pre>
<p>Similar to the jump from simple to multiple regression, adding in the <code>av</code> (most valuable team player) categorical variable improved the model, even if not all predictors are significant.</p>
<p>Next let’s see what happens when we remove the intercept from the model and the number of teams as that was one of the higher p-values once we added in <code>av</code>.</p>
<pre class="r"><code>mlr_model2 &lt;- update(mlr_model2, ~ . - 1 - num.teams) 

summary(mlr_model2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = wins ~ pf + pa + tak.giv.rnk + av - 1, data = lions)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.8851 -0.5596  0.0000  0.2408  2.8164 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## pf           0.024026   0.002897   8.293 4.40e-11 ***
## pa          -0.023556   0.002827  -8.333 3.80e-11 ***
## tak.giv.rnk -0.083701   0.034506  -2.426 0.018787 *  
## av           5.876636   0.712335   8.250 5.14e-11 ***
## avAllen      7.679832   1.748730   4.392 5.55e-05 ***
## avBackus     9.015964   1.685920   5.348 2.02e-06 ***
## avBaker      7.601480   1.587570   4.788 1.44e-05 ***
## avBarney     6.473658   0.994642   6.509 3.01e-08 ***
## avBly        8.612497   1.670937   5.154 4.01e-06 ***
## avBoyd       9.441635   1.616067   5.842 3.41e-07 ***
## avBrown      4.163210   1.624036   2.563 0.013294 *  
## avCofer      6.924300   1.542248   4.490 3.99e-05 ***
## avCrockett  10.639332   1.614382   6.590 2.23e-08 ***
## avCrowell    8.374441   1.644369   5.093 4.97e-06 ***
## avDanielson  8.820284   1.796609   4.909 9.45e-06 ***
## avEnglish    6.854108   1.302396   5.263 2.73e-06 ***
## avFlanagan   6.450239   1.293471   4.987 7.21e-06 ***
## avFreitas    7.439769   1.655773   4.493 3.94e-05 ***
## avGolladay   6.780133   1.879733   3.607 0.000695 ***
## avGriffin    8.089103   1.702236   4.752 1.63e-05 ***
## avHipple     9.835827   1.741163   5.649 6.86e-07 ***
## avHunter     8.376351   1.461175   5.733 5.07e-07 ***
## avJohnson    7.631709   1.510811   5.051 5.75e-06 ***
## avKarras     6.837586   1.476101   4.632 2.46e-05 ***
## avKitna      9.021719   1.618325   5.575 8.96e-07 ***
## avLandry     6.799698   1.704921   3.988 0.000209 ***
## avLane       8.260055   1.541757   5.358 1.95e-06 ***
## avLary       7.768686   1.538731   5.049 5.81e-06 ***
## avMoore      8.056106   1.527613   5.274 2.63e-06 ***
## avMunson     7.795381   1.529913   5.095 4.93e-06 ***
## avRogers     8.250364   1.386546   5.950 2.31e-07 ***
## avSanders    9.107666   1.294112   7.038 4.31e-09 ***
## avSims       7.970183   1.503357   5.302 2.38e-06 ***
## avStafford   9.113021   1.422623   6.406 4.38e-08 ***
## avStewart    8.196889   1.848965   4.433 4.82e-05 ***
## avStudstill  7.605984   1.548235   4.913 9.34e-06 ***
## avSuh        8.686199   1.428886   6.079 1.44e-07 ***
## avWalker     7.129878   1.265872   5.632 7.28e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.258 on 52 degrees of freedom
## Multiple R-squared:   0.98,  Adjusted R-squared:  0.9654 
## F-statistic: 67.17 on 38 and 52 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Now the model is cruising with an adjusted <span class="math inline">\(R^2\)</span> of 0.9654 and all p-values less implying significance.</p>
<p>It is difficult to predict who will be the team’s highest raned <code>av</code> player each year though so now lets add in a variable we can factor in with more certainty: the coach.</p>
<pre class="r"><code>mlr_model3 &lt;- update(mlr_model2, ~. + coaches  )

summary(mlr_model3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = wins ~ pf + pa + tak.giv.rnk + av + coaches - 1, 
##     data = lions)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.7057 -0.4133  0.0000  0.0000  2.7295 
## 
## Coefficients: (7 not defined because of singularities)
##                          Estimate Std. Error t value Pr(&gt;|t|)    
## pf                       0.026490   0.005192   5.102 1.36e-05 ***
## pa                      -0.024032   0.005620  -4.276 0.000153 ***
## tak.giv.rnk             -0.082604   0.045340  -1.822 0.077547 .  
## av                       5.391559   2.064466   2.612 0.013457 *  
## avAllen                  7.107003   3.746187   1.897 0.066591 .  
## avBackus                11.814157   3.289182   3.592 0.001054 ** 
## avBaker                  6.846889   3.580554   1.912 0.064559 .  
## avBarney                 5.734170   1.978961   2.898 0.006632 ** 
## avBly                    8.104807   3.399403   2.384 0.023023 *  
## avBoyd                   8.823768   2.722850   3.241 0.002724 ** 
## avBrown                  3.476330   2.670249   1.302 0.201974    
## avCofer                  6.520357   2.501734   2.606 0.013629 *  
## avCrockett               9.087825   2.964357   3.066 0.004310 ** 
## avCrowell                7.728333   2.820335   2.740 0.009830 ** 
## avDanielson              8.114636   4.046629   2.005 0.053191 .  
## avEnglish                6.120841   3.141452   1.948 0.059911 .  
## avFlanagan               5.745207   2.537473   2.264 0.030262 *  
## avFreitas                6.732767   2.786601   2.416 0.021382 *  
## avGolladay               7.131146   3.808640   1.872 0.070041 .  
## avGriffin               10.549277   4.123062   2.559 0.015283 *  
## avHipple                12.192703   4.190811   2.909 0.006437 ** 
## avHunter                 8.034518   2.201824   3.649 0.000900 ***
## avJohnson                9.365280   3.366530   2.782 0.008866 ** 
## avKarras                 6.342015   2.225502   2.850 0.007484 ** 
## avKitna                 10.122044   3.664751   2.762 0.009315 ** 
## avLandry                 6.079281   2.834544   2.145 0.039432 *  
## avLane                   7.708915   2.457524   3.137 0.003581 ** 
## avLary                   7.074675   2.367717   2.988 0.005267 ** 
## avMoore                  6.355498   3.228950   1.968 0.057483 .  
## avMunson                 5.387231   2.836307   1.899 0.066284 .  
## avRogers                 7.719152   2.726193   2.831 0.007834 ** 
## avSanders                7.757055   2.879939   2.693 0.011027 *  
## avSims                   7.018268   3.706213   1.894 0.067067 .  
## avStafford               8.984848   2.956890   3.039 0.004623 ** 
## avStewart               11.931173   3.932884   3.034 0.004682 ** 
## avStudstill              7.032582   3.164083   2.223 0.033206 *  
## avSuh                    9.440302   2.651409   3.560 0.001149 ** 
## avWalker                 6.492084   2.495222   2.602 0.013778 *  
## coachesClark             0.176372   1.343606   0.131 0.896360    
## coachesDorais           -0.198416   1.062210  -0.187 0.852965    
## coachesEdwards           0.502467   1.766458   0.284 0.777844    
## coachesFontes            0.944283   1.194926   0.790 0.435028    
## coachesForzano           1.898318   1.628428   1.166 0.252078    
## coachesGilmer            0.202568   2.007762   0.101 0.920247    
## coachesGriffen          -1.101958   1.741362  -0.633 0.531216    
## coachesHenderson         1.033090   1.805526   0.572 0.571076    
## coachesHudspeth                NA         NA      NA       NA    
## coachesHudspeth,Forzano -0.726554   1.623283  -0.448 0.657378    
## coachesKarcis,Edwards    0.748289   1.898122   0.394 0.695950    
## coachesMarinelli        -1.727028   2.429084  -0.711 0.482092    
## coachesMariucci          0.007601   1.994331   0.004 0.996982    
## coachesMariucci,Jauron         NA         NA      NA       NA    
## coachesMcCafferty        0.085885   2.052421   0.042 0.966874    
## coachesMcMillin          0.029188   1.187507   0.025 0.980538    
## coachesMornhinweg       -4.298652   2.350446  -1.829 0.076470 .  
## coachesParker           -0.015158   1.065768  -0.014 0.988738    
## coachesPatricia         -1.016049   1.664389  -0.610 0.545736    
## coachesRogers           -2.959805   2.449088  -1.209 0.235431    
## coachesRogers,Fontes           NA         NA      NA       NA    
## coachesRoss                    NA         NA      NA       NA    
## coachesRoss,Moeller            NA         NA      NA       NA    
## coachesSchmidt                 NA         NA      NA       NA    
## coachesSchwartz         -2.239289   1.277768  -1.753 0.088976 .  
## coachesWilson                  NA         NA      NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.402 on 33 degrees of freedom
## Multiple R-squared:  0.9843, Adjusted R-squared:  0.9571 
## F-statistic: 36.25 on 57 and 33 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>When coaches are input, the model stays pretty similar, however we are started to get some <code>NA</code> values for factor levels. The large number of factor levels from multiple dummy variables is starting to overburden the model, resulting in NA values. We will now move on to the next major subset of statistical modeling.</p>
</div>
</div>
<div id="classification-models" class="section level3">
<h3>Classification models</h3>
<p>Classification models use data to predict if observations of data will fall will fall into a given class of a categorical variable.</p>
<div id="logistic-regression" class="section level4">
<h4>Logistic Regression</h4>
<p>Logistic regression seeks to model the probability that the response variable belongs to a certain category. To do this is calculate the <span class="math inline">\(p(X) = P(Y = 1|X)\)</span> (where <span class="math inline">\(Y\)</span> is the response and <span class="math inline">\(X\)</span> is the predictor(s)) as follows:</p>
<p><span class="math display">\[p(X) = \frac{e^{\beta_0 +\beta_1X_1+...+\beta_pX_p}}{1+e^{\beta_0 +\beta_1X_1+...+\beta_pX_p}}\]</span></p>
<p>To begin we first take a look at how closely correlated each numeric variable is with one another. Values closer to 1 translate to a higher level of positive correlation, values closer to -1 translate to higher levels of negative correlation, and values close to zero signifiy less of any sort of a relationship between variables.</p>
<pre class="r"><code>cor(lions[,-c(5, 6, 10:14,28)])</code></pre>
<pre><code>##                    year        wins       loss        ties          pf
## year         1.00000000  0.02422207  0.6049591 -0.37888534  0.68875042
## wins         0.02422207  1.00000000 -0.6972187 -0.11819108  0.46009594
## loss         0.60495907 -0.69721872  1.0000000 -0.28270344  0.11626528
## ties        -0.37888534 -0.11819108 -0.2827034  1.00000000 -0.28238320
## pf           0.68875042  0.46009594  0.1162653 -0.28238320  1.00000000
## pa           0.81882948 -0.36075704  0.8274041 -0.33103366  0.53511193
## pd          -0.31512380  0.81966054 -0.8353195  0.12388629  0.28438658
## off.pts.rnk  0.68213869 -0.39497244  0.7711756 -0.27089046  0.06051458
## off.yds.rnk  0.66409488 -0.23873155  0.6249229 -0.28034439  0.12478406
## def.pts.rnk  0.75432999 -0.38546856  0.7975118 -0.29195437  0.37177165
## def.yds.rnk  0.75128498 -0.27210205  0.6940538 -0.32134709  0.43430023
## tak.giv.rnk  0.67041659 -0.44421591  0.7887655 -0.24792986  0.27934610
## pts.df.rnk   0.73578241 -0.48176487  0.8748723 -0.29216659  0.18191014
## yds.df.rnk   0.70324270 -0.33026878  0.7243637 -0.29871209  0.20345847
## num.teams    0.94594291  0.05445771  0.5951429 -0.39991527  0.64098885
## mov         -0.27890518  0.80832739 -0.8029231  0.12261738  0.30607435
## sos         -0.02181352 -0.24083450  0.1191762  0.17823230 -0.13029567
## srs         -0.29760663  0.77117595 -0.8023393  0.18139347  0.28049969
## osrs        -0.10037025  0.61255484 -0.5371345  0.08078409  0.48770008
## dsrs        -0.37671516  0.64809806 -0.7667888  0.20911924 -0.01046073
##                     pa         pd off.pts.rnk off.yds.rnk def.pts.rnk
## year         0.8188295 -0.3151238  0.68213869  0.66409488  0.75432999
## wins        -0.3607570  0.8196605 -0.39497244 -0.23873155 -0.38546856
## loss         0.8274041 -0.8353195  0.77117558  0.62492286  0.79751184
## ties        -0.3310337  0.1238863 -0.27089046 -0.28034439 -0.29195437
## pf           0.5351119  0.2843866  0.06051458  0.12478406  0.37177165
## pa           1.0000000 -0.6577212  0.64656281  0.58945361  0.84825948
## pd          -0.6577212  1.0000000 -0.67980058 -0.55768272 -0.63116149
## off.pts.rnk  0.6465628 -0.6798006  1.00000000  0.87566219  0.65697933
## off.yds.rnk  0.5894536 -0.5576827  0.87566219  1.00000000  0.61848420
## def.pts.rnk  0.8482595 -0.6311615  0.65697933  0.61848420  1.00000000
## def.yds.rnk  0.8154408 -0.5381624  0.56136768  0.58590451  0.88288612
## tak.giv.rnk  0.7564282 -0.6093583  0.69073254  0.50733815  0.77290182
## pts.df.rnk   0.8080389 -0.7548095  0.90502285  0.80064591  0.87321697
## yds.df.rnk   0.7207630 -0.6365496  0.81058678  0.89592423  0.78744333
## num.teams    0.7461876 -0.2752727  0.68604093  0.69504309  0.77007952
## mov         -0.6308738  0.9888701 -0.62164656 -0.51076421 -0.57570058
## sos          0.1280757 -0.2615282 -0.01607028  0.03461325  0.04345682
## srs         -0.6194969  0.9531549 -0.65459200 -0.52335009 -0.58816204
## osrs        -0.2890494  0.7628960 -0.62725039 -0.48077232 -0.17138367
## dsrs        -0.7093212  0.7956539 -0.45027957 -0.37906815 -0.76732010
##             def.yds.rnk  tak.giv.rnk  pts.df.rnk  yds.df.rnk   num.teams
## year         0.75128498  0.670416594  0.73578241  0.70324270  0.94594291
## wins        -0.27210205 -0.444215906 -0.48176487 -0.33026878  0.05445771
## loss         0.69405376  0.788765485  0.87487228  0.72436375  0.59514291
## ties        -0.32134709 -0.247929863 -0.29216659 -0.29871209 -0.39991527
## pf           0.43430023  0.279346099  0.18191014  0.20345847  0.64098885
## pa           0.81544083  0.756428188  0.80803892  0.72076302  0.74618765
## pd          -0.53816235 -0.609358276 -0.75480950 -0.63654956 -0.27527266
## off.pts.rnk  0.56136768  0.690732543  0.90502285  0.81058678  0.68604093
## off.yds.rnk  0.58590451  0.507338148  0.80064591  0.89592423  0.69504309
## def.pts.rnk  0.88288612  0.772901816  0.87321697  0.78744333  0.77007952
## def.yds.rnk  1.00000000  0.643925229  0.78629212  0.83070496  0.73152477
## tak.giv.rnk  0.64392523  1.000000000  0.79325541  0.60294652  0.65752573
## pts.df.rnk   0.78629212  0.793255414  1.00000000  0.88498934  0.73926657
## yds.df.rnk   0.83070496  0.602946519  0.88498934  1.00000000  0.72301133
## num.teams    0.73152477  0.657525732  0.73926657  0.72301133  1.00000000
## mov         -0.49071502 -0.555065648 -0.68710855 -0.57944924 -0.23766636
## sos          0.03561121  0.005025677  0.02631915  0.01910519 -0.10410120
## srs         -0.50142161 -0.578565607 -0.70992162 -0.59957446 -0.27992712
## osrs        -0.12182917 -0.317786445 -0.47084145 -0.38408860 -0.04863702
## dsrs        -0.67722744 -0.617538340 -0.68240642 -0.58861491 -0.39600796
##                    mov          sos          srs        osrs        dsrs
## year        -0.2789052 -0.021813520 -0.297606632 -0.10037025 -0.37671516
## wins         0.8083274 -0.240834495  0.771175949  0.61255484  0.64809806
## loss        -0.8029231  0.119176245 -0.802339318 -0.53713453 -0.76678881
## ties         0.1226174  0.178232296  0.181393465  0.08078409  0.20911924
## pf           0.3060743 -0.130295671  0.280499686  0.48770008 -0.01046073
## pa          -0.6308738  0.128075712 -0.619496898 -0.28904945 -0.70932121
## pd           0.9888701 -0.261528235  0.953154854  0.76289598  0.79565393
## off.pts.rnk -0.6216466 -0.016070278 -0.654592000 -0.62725039 -0.45027957
## off.yds.rnk -0.5107642  0.034613249 -0.523350093 -0.48077232 -0.37906815
## def.pts.rnk -0.5757006  0.043456817 -0.588162037 -0.17138367 -0.76732010
## def.yds.rnk -0.4907150  0.035611207 -0.501421608 -0.12182917 -0.67722744
## tak.giv.rnk -0.5550656  0.005025677 -0.578565607 -0.31778644 -0.61753834
## pts.df.rnk  -0.6871085  0.026319151 -0.709921625 -0.47084145 -0.68240642
## yds.df.rnk  -0.5794492  0.019105187 -0.599574463 -0.38408860 -0.58861491
## num.teams   -0.2376664 -0.104101202 -0.279927115 -0.04863702 -0.39600796
## mov          1.0000000 -0.286876455  0.956866208  0.77844415  0.78721170
## sos         -0.2868765  1.000000000  0.003712419 -0.03527244  0.03932978
## srs          0.9568662  0.003712419  1.000000000  0.80203330  0.83356415
## osrs         0.7784442 -0.035272440  0.802033303  1.00000000  0.33867158
## dsrs         0.7872117  0.039329775  0.833564153  0.33867158  1.00000000</code></pre>
<p>With there being a large number of different variables it is slightly difficult to process, but this map does give a general idea of the relationship between variables.</p>
<p>In order to carry out logistic regression we will need to frame our problem differently. The response variable now needs to be a binary factor variable. However, all of our categorical variables have more than two levels.</p>
<p>To get a binary categorial variable we will translate the number of wins in a season into our response variable. In today’s NFL, a team needs to win 10 games or more to have a reasonable chance of making the playoffs. We will therefore make a new variable with two levels: ten wins or more, and less than ten wins.</p>
<pre class="r"><code>lions &lt;- lions %&gt;%
  mutate(win.threshold = as.factor(ifelse(wins &gt;= 10, &quot;Playoffs&quot;, &quot;No-playoffs&quot;)))

attach(lions)</code></pre>
<p>Now that we have our new respsponse variable <code>wins.threshold</code> we can apply the logistic regression model.</p>
<pre class="r"><code>log_reg_model &lt;- glm(win.threshold ~ pf + pa + tak.giv.rnk + mov + sos , lions, family = binomial)

summary(log_reg_model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = win.threshold ~ pf + pa + tak.giv.rnk + mov + sos, 
##     family = binomial, data = lions)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.12955  -0.23213  -0.07259  -0.00186   3.03972  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept) -3.12433    2.22416  -1.405  0.16010   
## pf           0.25742    0.09182   2.803  0.00506 **
## pa          -0.25172    0.09244  -2.723  0.00647 **
## tak.giv.rnk -0.34736    0.16173  -2.148  0.03173 * 
## mov         -3.00164    1.12183  -2.676  0.00746 **
## sos         -0.46710    0.30876  -1.513  0.13033   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 62.790  on 89  degrees of freedom
## Residual deviance: 24.658  on 84  degrees of freedom
## AIC: 36.658
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<p>Now that we have a logistic model we can calculate a table to represent how accurate the model is.</p>
<pre class="r"><code>contrasts(win.threshold)</code></pre>
<pre><code>##             Playoffs
## No-playoffs        0
## Playoffs           1</code></pre>
<pre class="r"><code>glm_prob &lt;- predict(log_reg_model, type = &quot;response&quot;)
glm_pred &lt;- rep(&quot;No-playoffs&quot;, nrow(lions))
glm_pred[glm_prob &gt; 0.5] = &quot;Playoffs&quot;

table(glm_pred, win.threshold)</code></pre>
<pre><code>##              win.threshold
## glm_pred      No-playoffs Playoffs
##   No-playoffs          80        4
##   Playoffs              0        6</code></pre>
<pre class="r"><code>mean(glm_pred == win.threshold)</code></pre>
<pre><code>## [1] 0.9555556</code></pre>
<p>Our model correctly predicted reality 95.6% of the time. It predicted 80 seasons correctly where the team did not win 10 games, 6 seasons correctly where they won 10 games or more, and only 4 seasons incorrectly where they were not predicted to make the threshold, but did.</p>
<p>To better shape the model, we will define a training set to base our model on, and then test this new model on the testing set (the observations not included in the training set).</p>
<pre class="r"><code>train &lt;- (year &lt; length(year) * 0.8 + min(year))
test &lt;- !train

log_reg_model2 &lt;- glm(win.threshold ~ pf + pa + tak.giv.rnk + mov + sos , lions, family = binomial, subset = train)

glm_prob2 &lt;- predict(log_reg_model2, lions[test,], type = &quot;response&quot;)
glm_pred2 &lt;- rep(&quot;No-playoffs&quot;, nrow(lions[test,]))
glm_pred2[glm_prob2 &gt; 0.5] = &quot;Playoffs&quot;

table(glm_pred2, win.threshold[test])</code></pre>
<pre><code>##              
## glm_pred2     No-playoffs Playoffs
##   No-playoffs          16        1
##   Playoffs              0        1</code></pre>
<pre class="r"><code>mean(glm_pred2 == win.threshold[test])</code></pre>
<pre><code>## [1] 0.9444444</code></pre>
<pre class="r"><code>mean(glm_pred2 != win.threshold[test])</code></pre>
<pre><code>## [1] 0.05555556</code></pre>
<p>The training data set is composed of the seasons between 1930 and 2002. The model than tests its efficacy on the seasons between 2003 and 2019. In the end the model held 94.4% accuracy (with a corresponding test error rate of 5.6%) in its predictions (with only a single missed prediction of a 9- win season).</p>
</div>
<div id="linear-discriminant-analysis-lda" class="section level4">
<h4>Linear Discriminant Analysis (LDA)</h4>
<p>The next method is LDA, which can offer more stability in the parameter estimates and model than logistic regression, and is more useful when there are more than two response classes.</p>
<p>Logistic regression takes the prior distributions (<span class="math inline">\(\pi_k\)</span>) of the <span class="math inline">\(k\)</span> classes of <span class="math inline">\(Y\)</span> and the density functions <span class="math inline">\((f_k(x))\)</span> of <span class="math inline">\(X\)</span> and then applies <em>Bayes’ Theorem</em> to calculate:</p>
<p><span class="math display">\[P(Y=k|X=x)=\frac{\pi_kf_k(x)}{\sum_{l=1}^K\pi_lf_l(x)}\]</span></p>
<p>To start the new LDA model, we will proceed with the same training set used in logistic regression.</p>
<pre class="r"><code>library(MASS)</code></pre>
<pre><code>## 
## Attaching package: &#39;MASS&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     select</code></pre>
<pre class="r"><code>lda_model &lt;- lda(win.threshold ~ pf + pa + tak.giv.rnk + mov + sos , data = lions, subset = train)

lda_model</code></pre>
<pre><code>## Call:
## lda(win.threshold ~ pf + pa + tak.giv.rnk + mov + sos, data = lions, 
##     subset = train)
## 
## Prior probabilities of groups:
## No-playoffs    Playoffs 
##   0.8888889   0.1111111 
## 
## Group means:
##                   pf       pa tak.giv.rnk        mov        sos
## No-playoffs 251.4844 261.2031    10.03125 -0.6171875  0.6109375
## Playoffs    302.3750 205.3750     3.50000  7.0125000 -0.2750000
## 
## Coefficients of linear discriminants:
##                     LD1
## pf           0.05026373
## pa          -0.04519235
## tak.giv.rnk -0.05458738
## mov         -0.50226966
## sos         -0.08848905</code></pre>
<p>Now carrying forward in a similar manner as before to test the accuracy of our model.</p>
<pre class="r"><code>lda_pred &lt;- predict(lda_model, lions[test,])
lda_class &lt;- lda_pred$class

table(lda_class, win.threshold[test])</code></pre>
<pre><code>##              
## lda_class     No-playoffs Playoffs
##   No-playoffs          16        1
##   Playoffs              0        1</code></pre>
<pre class="r"><code>mean(lda_class == win.threshold[test])</code></pre>
<pre><code>## [1] 0.9444444</code></pre>
<p>With the small data set and same form the model, the lda performs identically to the logistic regression model.</p>
</div>
<div id="quadratic-discriminant-analysis-qda" class="section level4">
<h4>Quadratic Discriminant Analysis (QDA)</h4>
<p>QDA works almost identically to LDA, major difference being that rather than a linear function of the predictors the QDA classifier involves quadratic.</p>
<pre class="r"><code>qda_model &lt;- qda(win.threshold ~ pf + pa + tak.giv.rnk + mov + sos , data = lions, subset = train)

qda_model</code></pre>
<pre><code>## Call:
## qda(win.threshold ~ pf + pa + tak.giv.rnk + mov + sos, data = lions, 
##     subset = train)
## 
## Prior probabilities of groups:
## No-playoffs    Playoffs 
##   0.8888889   0.1111111 
## 
## Group means:
##                   pf       pa tak.giv.rnk        mov        sos
## No-playoffs 251.4844 261.2031    10.03125 -0.6171875  0.6109375
## Playoffs    302.3750 205.3750     3.50000  7.0125000 -0.2750000</code></pre>
<pre class="r"><code>qda_class &lt;- predict(qda_model, lions[test,])$class
table(qda_class, win.threshold[test])</code></pre>
<pre><code>##              
## qda_class     No-playoffs Playoffs
##   No-playoffs          15        0
##   Playoffs              1        2</code></pre>
<pre class="r"><code>mean(qda_class == win.threshold[test])</code></pre>
<pre><code>## [1] 0.9444444</code></pre>
<p>Again, the model is basically unchanged. This is due to the small number of observations in the testing data set(18 seasons). The only interesting tidbit from the QDA model is that it correctly predicts both seasons with over 10 wins, while prediction an extra season over 10 wins that was actually under.</p>
</div>
<div id="k-nearest-neighbors-knn" class="section level4">
<h4>K-Nearest Neighbors (KNN)</h4>
<p>The last model that will be is the KNN model, which groups predictor variable data together into <span class="math inline">\(k\)</span> classes. First we will try with <span class="math inline">\(k = 1\)</span>.</p>
<pre class="r"><code>library(class)
train_x &lt;- cbind(pf, pa, tak.giv.rnk, mov, sos)[train,]
test_x &lt;- cbind(pf, pa, tak.giv.rnk, mov, sos)[test,]
train_win.threshold &lt;- win.threshold[train]

set.seed(1)
knn1_pred &lt;- knn(train_x, test_x, train_win.threshold, k = 1)
table(knn1_pred, win.threshold[test])</code></pre>
<pre><code>##              
## knn1_pred     No-playoffs Playoffs
##   No-playoffs          15        1
##   Playoffs              1        1</code></pre>
<pre class="r"><code>mean(knn1_pred == win.threshold[test])</code></pre>
<pre><code>## [1] 0.8888889</code></pre>
<p>With $k = 1 $ we can see that the model performs slightly worse, making two innacuracte predictions. Lets try with <span class="math inline">\(k = 3\)</span> and see if it improves the model.</p>
<pre class="r"><code>set.seed(1)
knn3_pred &lt;- knn(train_x, test_x, train_win.threshold, k = 3)
table(knn3_pred, win.threshold[test])</code></pre>
<pre><code>##              
## knn3_pred     No-playoffs Playoffs
##   No-playoffs          16        2
##   Playoffs              0        0</code></pre>
<pre class="r"><code>mean(knn3_pred == win.threshold[test])</code></pre>
<pre><code>## [1] 0.8888889</code></pre>
<p>It does not improve the modela nd the only difference becomes that the model predicts no seasons with 10+ wins. With the small amount of data to predict this is not totally unexpected.</p>
</div>
</div>
<div id="cross-validation" class="section level3">
<h3>Cross-Validation</h3>
<p>So far we have used only the most recent 20% of observations as the testing data. If we want to test the validity of our model though, it would be more effective to us random combinations of data, and the whole dataset as both testing and training sets in different iterations.</p>
<p>This is where cross-validation comes in. First the data will be randomized using the <code>sample</code> function:</p>
<pre class="r"><code>set.seed(1)
train &lt;- sample(90,90)

lions_cv &lt;- lions[train,]</code></pre>
<p>Now the only thing to do is run all of the models again and test the errors using 10 crossfold-validation. Also this time instead we will be using the mean absolute error as the judge of the models.</p>
<pre class="r"><code>MAE &lt;- c()

# logistic regression
mae &lt;- c()
for (i in 1:10) {
  train_data &lt;- lions_cv[-((i*9-8):(i*9)), ]
  test_data &lt;- lions_cv[((i*9-8):(i*9)), ]
  log &lt;- glm(win.threshold ~ pf + pa + tak.giv.rnk + mov + sos , data = train_data , family = binomial)
  log_probs &lt;- predict(log, test_data, type = &quot;response&quot;)
  log_pred &lt;- rep(&quot;No-playoffs&quot;, nrow(test_data))
  log_pred[log_probs &gt; 0.5] &lt;- &quot;Playoffs&quot;
  mae &lt;- c(mae, mean(abs(as.integer(log_pred == &quot;Playoffs&quot;) - as.integer(test_data$win.threshold == &quot;Playoffs&quot;))))
}
MAE &lt;- c(MAE, mean(mae))

# lda
mae &lt;- c()
for(i in 1:10) {
  train_data &lt;- lions_cv[-((i*9-8):(i*9)), ]
  test_data &lt;- lions_cv[((i*9-8):(i*9)), ]
  lda &lt;- lda(win.threshold ~ pf + pa + tak.giv.rnk + mov + sos , data = train_data)
  lda_pred &lt;- predict(lda, test_data)
  mae &lt;- c(mae, mean(abs(as.integer(lda_pred$class == &quot;Playoffs&quot;) - as.integer(test_data$win.threshold == &quot;Playoffs&quot;))))
}
MAE &lt;- c(MAE, mean(mae))

# qda
mae &lt;- c()
for(i in 1:10) {
  train_data &lt;- lions_cv[-((i*9-8):(i*9)), ]
  test_data &lt;- lions_cv[((i*9-8):(i*9)), ]
  qda &lt;- qda(win.threshold ~ pf + pa + tak.giv.rnk + mov + sos , data = train_data)
  qda_pred &lt;- predict(qda, test_data)
  mae &lt;- c(mae, mean(abs(as.integer(qda_pred$class == &quot;Playoffs&quot;) - as.integer(test_data$win.threshold == &quot;Playoffs&quot;))))
}
MAE &lt;- c(MAE,mean(mae))

# knn k = 3
train_x &lt;- cbind(pf, pa, tak.giv.rnk, mov, sos)[-((i*9-8):(i*9)), ]
test_x &lt;- cbind(pf, pa, tak.giv.rnk, mov, sos)[((i*9-8):(i*9)), ]
train_win.threshold &lt;- win.threshold[-((i*9-8):(i*9))]

mae &lt;- c()
for(i in 1:10) {
  train_data &lt;- lions_cv[-((i*9-8):(i*9)), ]
  test_data &lt;- lions_cv[((i*9-8):(i*9)), ]
  knn3_pred &lt;- knn(train_x, test_x, train_win.threshold, k = 3)
  mae &lt;- c(mae, mean(abs(as.integer(knn3_pred == &quot;Playoffs&quot;) - as.integer(test_data$win.threshold == &quot;Playoffs&quot;))))
}
MAE &lt;- c(MAE, mean(mae))

MAE</code></pre>
<pre><code>## [1] 0.08888889 0.12222222 0.07777778 0.11111111</code></pre>
<p><img src="/analytics/2019-12-12-methods-of-regression_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Cross validation is extremely useful in that it uses all portions of the data to both test and train, and then computes the average error based on each individual sampling. From the graph above we can see that the QDA model had the lowest error mean absolute error (making it the most accurate) while the LDA model had the highest (making it the least accurate).</p>
<p>Understanding traditional statistical methods are important as they will help us better comprehend machine learning in R in the next post.</p>
</div>
<div id="reference" class="section level3">
<h3>Reference</h3>
<p>I first learned the techniques showcased in this post from <em>An Introduction to Statistical Learning: with Applications in R</em>.</p>
</div>

    

    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    
  </div>
  

<div class="navigation navigation-single">
    
    
    <a href="/analytics/web-scrape/" class="navigation-next">
      <span class="navigation-tittle">Web Scraping</span>
      <i aria-hidden="true" class="fa fa-chevron-right"></i>
    </a>
    
</div>


  

  
    


</article>


        </div>
        
    

<script defer src="https://use.fontawesome.com/releases/v5.11.2/js/all.js" integrity="sha384-b3ua1l97aVGAPEIe48b4TC60WUQbQaGi2jqAWM90y0OZXZeyaTCWtBTKtjW2GXG1" crossorigin="anonymous"></script>


    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
        
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    



    



    </body>
</html>
