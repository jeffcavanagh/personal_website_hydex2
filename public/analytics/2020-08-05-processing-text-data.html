<script src="2020-08-05-processing-text-data_files/header-attrs-2.6/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#setup">Setup</a></li>
<li><a href="#transforming-raw-text-into-data-frame">Transforming Raw Text into Data Frame</a></li>
<li><a href="#text-analysis">Text Analysis</a>
<ul>
<li><a href="#counts">Counts</a></li>
<li><a href="#document-frequencies">Document Frequencies</a></li>
</ul></li>
</ul>
</div>

<p><strong>Text data cannot be processed identically to numerical or categorical data. Is there a way then to analyze text data and obtain insight from it??</strong></p>
<p>Of course! All it takes is an adapted approach and a little more work upfront.</p>
<p>Quite a bit of potential data is unstructured and textual in nature.To extract the underlying value of this category of data, different techniques are required.</p>
<p>In this post we will focus on some routine methods for text mining and text analysis in R and how they can be utilized to process and draw value from unstructured text data.</p>
<div id="setup" class="section level3">
<h3>Setup</h3>
<p>In R, the first stop is the <code>stringr</code> package. This package can be loaded on its own, or as part of <code>tidyverse</code>.</p>
<p>The functions contained within process character strings in various ways. Luckily for us, R has a <a href="/rcheatsheets/strings.pdf" target="blank">cheatsheet</a> for the package.</p>
<p>It should be noted that there is also the <code>stringi</code> package which contains more robust functions for handling strings. For simplicity, we will only be concentrating on <code>stringr</code> in this post.</p>
<pre class="r"><code>library(stringr)
library(tidyverse)</code></pre>
<p>To illustrate the use of text manipulation in R, we will be analyzing a pdf of Mary Shelley’s <a href="/analytics/2020-08-05-processing-text-data_files/frankenstein.pdf" target="blank"><em>Frankenstein</em></a> (downloaded from <a href="https://www.planetebook.com/free-ebooks/frankenstein.pdf" target="blank">planetbook.com</a>). We will use the <code>pdftools</code> package to extract the text from the pdf file.</p>
<pre class="r"><code>library(pdftools)
book &lt;- pdf_text(&quot;frankenstein.pdf&quot;)</code></pre>
<p><code>pdf_text</code> converts the pdf file into a character string where each element is a page from the document.</p>
<p>Now that we have defined the book as a character string we will begin to mold it into a data frame so it is easier to work with.</p>
<pre class="r"><code>book_df &lt;- book  %&gt;% 
  as_tibble_col(&quot;content&quot;) %&gt;%
  rowid_to_column(&quot;page_number&quot;)

book_df</code></pre>
<pre><code>## # A tibble: 277 x 2
##    page_number content                                                          
##          &lt;int&gt; &lt;chr&gt;                                                            
##  1           1 &quot;Frankenstein\nBy Mary Wollstonecraft Shelley\nDownload free eBo…
##  2           2 &quot;Letter 1\nTo Mrs. Saville, England\n   St. Petersburgh, Dec. 11…
##  3           3 &quot;and features may be without example, as the phenomena of\nthe h…
##  4           4 &quot;North Pacific Ocean through the seas which surround the\npole. …
##  5           5 &quot;practical advantage. Twice I actually hired myself as an un-\nd…
##  6           6 &quot;easily be done by paying the insurance for the owner, and\nto e…
##  7           7 &quot;Letter 2\nTo Mrs. Saville, England\n   Archangel, 28th March, 1…
##  8           8 &quot;to me that I am self-educated: for the first fourteen years\nof…
##  9           9 &quot;remarkable in the ship for his gentleness and the mildness of\n…
## 10          10 &quot;himself bound in honour to my friend, who, when he found\nthe f…
## # … with 267 more rows</code></pre>
<p>This gives a data frame where each row is an observation containing the page number and content held on that page. Now we can begin in earnest with the text mining.</p>
<ul>
<li><code>str_detect</code> is used to detect which observations match a pattern</li>
<li><code>str_which</code> is used to pick out the index’s of the entries of all matches</li>
<li><code>str_extract</code> is used to pull out the first pattern match within each element
<ul>
<li>Some functions in <code>stringr</code> also have a <code>_all</code> functionality, such as <code>str_extract_all</code>, which returns all matches within an observation</li>
</ul></li>
</ul>
<p>Here is a look at these three function in practice. To illustrate the use of these functions, we will search for all pages that reference “Frankenstein”.</p>
<pre class="r"><code>book_df$content %&gt;%
  str_detect(&quot;Frankenstein&quot;)</code></pre>
<pre><code>##   [1]  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
##  [13] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
##  [25] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
##  [37] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
##  [49] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
##  [61]  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
##  [73]  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE
##  [85] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
##  [97] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
## [109] FALSE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE
## [121] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
## [133] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
## [145] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
## [157] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
## [169] FALSE  TRUE  TRUE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
## [181] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
## [193] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
## [205] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
## [217] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
## [229] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
## [241] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
## [253] FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE  TRUE
## [265]  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE
## [277] FALSE</code></pre>
<pre class="r"><code>book_df$content %&gt;%
  str_detect(&quot;Frankenstein&quot;) %&gt;%
  sum()</code></pre>
<pre><code>## [1] 147</code></pre>
<pre class="r"><code>book_df$content %&gt;%
  str_detect(&quot;Frankenstein&quot;) %&gt;%
  mean()</code></pre>
<pre><code>## [1] 0.5306859</code></pre>
<p>Adding on the <code>sum</code> and <code>mean</code> functions we are also able to see the number and percentage of pages which contain the “Frankenstein” text pattern.</p>
<p>Next, <code>str_which</code> gives us the locations of the pages containing the pattern.</p>
<pre class="r"><code>ind_capitals &lt;- book_df$content %&gt;%
  str_which(&quot;Frankenstein&quot;)

ind_capitals</code></pre>
<pre><code>##   [1]   1   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34
##  [19]  36  38  40  42  44  46  48  50  52  54  56  58  60  61  62  64  66  68
##  [37]  70  72  73  74  76  78  79  80  82  84  86  88  90  92  94  96  98 100
##  [55] 102 104 106 108 110 112 114 115 116 118 120 122 124 126 128 130 132 134
##  [73] 136 138 140 142 144 146 148 150 152 154 156 158 160 162 164 166 168 170
##  [91] 171 172 174 176 178 180 182 184 186 188 190 192 194 196 198 200 202 204
## [109] 206 208 210 212 214 216 218 220 222 224 226 228 230 232 234 236 238 240
## [127] 242 244 246 248 250 252 254 256 258 260 262 264 265 266 268 270 271 272
## [145] 273 274 276</code></pre>
<p>Lastly, <code>str_extract</code> pulls the pattern from each of the pages. However, <code>str_extract</code> will return a vector as the same length as the input. In order to filter to only entries that have the pattern match, we will combine it with <code>str_which</code>.</p>
<pre class="r"><code>book_df$content[ind_capitals] %&gt;%
  str_extract(&quot;Frankenstein&quot;)</code></pre>
<pre><code>##   [1] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##   [6] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [11] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [16] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [21] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [26] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [31] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [36] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [41] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [46] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [51] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [56] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [61] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [66] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [71] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [76] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [81] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [86] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [91] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
##  [96] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
## [101] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
## [106] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
## [111] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
## [116] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
## [121] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
## [126] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
## [131] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
## [136] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
## [141] &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot; &quot;Frankenstein&quot;
## [146] &quot;Frankenstein&quot; &quot;Frankenstein&quot;</code></pre>
<p>Some other characters to take note of: <code>^</code> is used to search for matches at the start of a string, <code>$</code> is used to search for matches at the end of a string, <code>.</code> is used to match any character that is not a new line, <code>[:upper:]</code> is used to match upper case words, <code>*</code> is used to pull 0 or more matches, and <code>+</code> is used to pull 1 or more matches.</p>
</div>
<div id="transforming-raw-text-into-data-frame" class="section level3">
<h3>Transforming Raw Text into Data Frame</h3>
<p>The text data is now in a data frame, but is still far from orderly. Perusing through a few pages reveals that the footer at the bottom of each page is either the page number (which we already have) and the title of the book (Frankenstein), or the page number and the website home of the pdf.</p>
<p>Neither of these are useful to us. Let’s use <code>str_remove</code> to remove these from the pages.</p>
<pre class="r"><code>book_df$content &lt;- book_df$content %&gt;%
  str_remove(&quot;\\n(\\d+|\\030)\\s+Frankenstein\n$|\\n\\s*Free eBooks at Planet eBook.com\\s+(\\d+|\\\030)\\n$&quot;)</code></pre>
<ul>
<li><code>\\n</code> is used to detect new lines</li>
<li><code>\\d</code> is used to detect digits</li>
<li><code>\\s</code> is used to detect and spaces</li>
<li><code>\\\</code> is used to detect a back slash</li>
<li><code>|</code> is used as an or statement to detect multiple patterns</li>
</ul>
<p>Now our strings contain only the most relevant information, but they are still dense and difficult to decipher insight from.</p>
<p>The next step is to use the <code>tidytext</code> package to further break up the data. Using the <code>unnest_token</code> function it is possible to breakdown character strings into lines, paragraphs, sentences, words, and n-grams. First we will update the data frame by breaking the strings down by line.</p>
<pre class="r"><code>library(tidytext)

book_df &lt;- book_df %&gt;%
  unnest_tokens(&quot;lines&quot;, content, token = &quot;lines&quot;, to_lower = F)

book_df</code></pre>
<pre><code>## # A tibble: 7,578 x 2
##    page_number lines                                                         
##          &lt;int&gt; &lt;chr&gt;                                                         
##  1           1 &quot;Frankenstein&quot;                                                
##  2           1 &quot;By Mary Wollstonecraft Shelley&quot;                              
##  3           1 &quot;Download free eBooks of classic literature, books and&quot;       
##  4           1 &quot;novels at Planet eBook. Subscribe to our free eBooks blog&quot;   
##  5           1 &quot;and email newsletter.&quot;                                       
##  6           2 &quot;Letter 1&quot;                                                    
##  7           2 &quot;To Mrs. Saville, England&quot;                                    
##  8           2 &quot;   St. Petersburgh, Dec. 11th, 17—&quot;                          
##  9           2 &quot;   You will rejoice to hear that no disaster has accompanied&quot;
## 10           2 &quot;the commencement of an enterprise which you have regard-&quot;    
## # … with 7,568 more rows</code></pre>
<p>Now that we have the text broken up into lines it is much easier to process. Next let’s add more identifying variables where each line can be identified by line number and the chapter.</p>
<p><em>Frankenstein</em> begins with several letters before it gets into the chapters. To identify which pages fall under letters or chapters we need to do a little preliminary work with the tools demonstrated so far.</p>
<pre class="r"><code>ind_chapter &lt;- book_df$lines %&gt;%
  str_which(&quot;^Chapter&quot;) %&gt;%
  append(nrow(book_df))

chapters &lt;- book_df$lines[ind_chapter[-length(ind_chapter)]] %&gt;%
  str_extract(&quot;^Chapter.*&quot;) %&gt;%
  unlist()

ind_letter &lt;- book_df$lines %&gt;%
  str_which(&quot;^Letter&quot;) %&gt;%
  append(ind_chapter[1])

letters &lt;- book_df$lines[ind_letter[-length(ind_letter)]] %&gt;%
  str_extract_all(&quot;^Letter.*&quot;) %&gt;%
  unlist()</code></pre>
<p>Now that we have our indexes and values we can create new columns to further classify each observation.</p>
<pre class="r"><code>book_df &lt;- book_df %&gt;%
  mutate(
    line_number = row_number(),
    chapter = &quot;fill in&quot;
  )
book_df$chapter[c(1:(ind_letter[1] - 1))] &lt;- &quot;Preface&quot;

for (i in 1:(length(ind_letter) - 1)){
  book_df$chapter[c(ind_letter[i]:(ind_letter[i + 1] -1))] &lt;- letters[i]
}

for (i in 1:(length(ind_chapter) - 1)){
  book_df$chapter[c(ind_chapter[i]:(ind_chapter[i + 1] -1))] &lt;- chapters[i]
}

book_df &lt;- book_df %&gt;%
  select(chapter, page_number, line_number, lines)</code></pre>
<p><code>mutate</code> was used to add the new columns to the data frame and then <code>for</code> loops were used to set the values of the <em>chapter</em> column using the indexes and values that were established.</p>
<p>What began as a pdf of unstructured text is now a structured data frame with clear variables and observations. Going forward this will be much easier to work with.</p>
</div>
<div id="text-analysis" class="section level3">
<h3>Text Analysis</h3>
<div id="counts" class="section level4">
<h4>Counts</h4>
<p>Now that we have a structured data frame, lets look for the most common words that appear within the text.</p>
<pre class="r"><code>book_words &lt;- book_df %&gt;%
  unnest_tokens(word, lines, token = &quot;words&quot;) 

book_words %&gt;%
  count(word, sort = T)</code></pre>
<pre><code>## # A tibble: 7,895 x 2
##    word      n
##    &lt;chr&gt; &lt;int&gt;
##  1 the    4192
##  2 and    2977
##  3 i      2851
##  4 of     2649
##  5 to     2108
##  6 my     1780
##  7 a      1392
##  8 in     1155
##  9 was    1021
## 10 that   1017
## # … with 7,885 more rows</code></pre>
<p>The most common words are unsurprisingly the filler words found in any text. To sort these words out we will use the <code>stop_words</code> data frame, which is a list of these common words.</p>
<pre class="r"><code>book_words &lt;- book_words %&gt;%
  anti_join(stop_words) </code></pre>
<pre><code>## Joining, by = &quot;word&quot;</code></pre>
<pre class="r"><code>book_words %&gt;%
  count(word, sort = T)</code></pre>
<pre><code>## # A tibble: 7,396 x 2
##    word          n
##    &lt;chr&gt;     &lt;int&gt;
##  1 life        116
##  2 father      111
##  3 eyes        104
##  4 time         99
##  5 night        92
##  6 found        87
##  7 mind         84
##  8 day          82
##  9 elizabeth    82
## 10 heart        81
## # … with 7,386 more rows</code></pre>
<p>Now that we have this count, it would be nice to have a visualization of the most common words. Let’s do that now using <code>ggplot</code>.</p>
<pre class="r"><code>book_words %&gt;%
  count(word, sort = T) %&gt;%
  filter(n &gt; 60) %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  ggplot() +
  geom_col(aes(n, word))</code></pre>
<p><img src="2020-08-05-processing-text-data_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>It is also useful to count combinations of words. <code>tidytext</code> gives this option using <code>ngrams</code>.</p>
<pre class="r"><code>book_df %&gt;%
  unnest_tokens(bigram, lines, token = &quot;ngrams&quot;, n = 2) %&gt;%
  count(bigram, sort= T) # again the most common word pairings belong to the stop_words subset, we can still filter these out though</code></pre>
<pre><code>## # A tibble: 39,255 x 2
##    bigram      n
##    &lt;chr&gt;   &lt;int&gt;
##  1 of the    491
##  2 of my     254
##  3 in the    250
##  4 i was     219
##  5 i had     210
##  6 that i    194
##  7 and i     190
##  8 to the    180
##  9 and the   179
## 10 on the    144
## # … with 39,245 more rows</code></pre>
<pre class="r"><code>bigrams &lt;- book_df %&gt;%
  unnest_tokens(bigram, lines, token = &quot;ngrams&quot;, n = 2)

bigrams_sep &lt;- bigrams %&gt;%
  separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;)

bigrams_filter &lt;- bigrams_sep %&gt;%
  filter(!word1 %in% stop_words$word) %&gt;%
  filter(!word2 %in% stop_words$word)

bigrams_count &lt;- bigrams_filter %&gt;%
  count(word1, word2, sort = T) %&gt;% 
  na.omit()

bigrams_count</code></pre>
<pre><code>## # A tibble: 4,813 x 3
##    word1   word2          n
##    &lt;chr&gt;   &lt;chr&gt;      &lt;int&gt;
##  1 native  country       12
##  2 fellow  creatures     11
##  3 natural philosophy    10
##  4 dear    victor         9
##  5 de      lacey          8
##  6 mont    blanc          7
##  7 poor    girl           7
##  8 poor    william        6
##  9 gentle  manners        5
## 10 native  town           5
## # … with 4,803 more rows</code></pre>
<p>This same technique can be applied to any number of word groupings by setting the value of <code>n</code>.</p>
<p>We are also able to produce a nice visualization of these groupings using a word network.</p>
<pre class="r"><code>library(grid)
library(igraph)</code></pre>
<pre><code>## Warning: package &#39;igraph&#39; was built under R version 3.6.2</code></pre>
<pre><code>## 
## Attaching package: &#39;igraph&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:dplyr&#39;:
## 
##     as_data_frame, groups, union</code></pre>
<pre><code>## The following objects are masked from &#39;package:purrr&#39;:
## 
##     compose, simplify</code></pre>
<pre><code>## The following object is masked from &#39;package:tidyr&#39;:
## 
##     crossing</code></pre>
<pre><code>## The following object is masked from &#39;package:tibble&#39;:
## 
##     as_data_frame</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     decompose, spectrum</code></pre>
<pre><code>## The following object is masked from &#39;package:base&#39;:
## 
##     union</code></pre>
<pre class="r"><code>library(ggraph)</code></pre>
<pre><code>## Warning: package &#39;ggraph&#39; was built under R version 3.6.2</code></pre>
<pre class="r"><code>set.seed(1)
bigrams_count %&gt;% 
  filter(n &gt; 3) %&gt;%
  graph_from_data_frame() %&gt;%
  ggraph(layout = &quot;fr&quot;) +
  geom_edge_link(show.legend = F,
                 arrow = arrow(type = &quot;closed&quot;, length = unit(0.1, &quot;inches&quot;)),
                 aes(edge_alpha = n)) +
  geom_node_point(color = &quot;green&quot;, size = 2) +
  geom_node_text(aes(label = name), repel = T) +
  theme_void()</code></pre>
<p><img src="2020-08-05-processing-text-data_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>In the above visualization arrows point from the first word to the second. The shading option (<code>edge_alpha</code>) included in the code makes an arrow darker the more times the word combination appears. We also added a filter that only allows word combinations which have more than 3 occurrences to appear in the network.</p>
</div>
<div id="document-frequencies" class="section level4">
<h4>Document Frequencies</h4>
<p>The last technique illustrated in this post concerns the frequency of text appearances within data. Word counts have there uses, but the number of appearances of a word or a grouping of words is not the best metric for how important those elements are to the overall data source.</p>
<p><code>tf</code> stands for term frequency and represents how often are chosen element (word, bigram, trigram, sentence, etc) appears in a document.</p>
<p><code>idf</code> stands for inverse document frequency and represents how unique an element is in a data set. It is calculated by the following formula:</p>
<p><span class="math display">\[idf(\text{element}) = ln(\frac{n_{\text{documents}}}{n_{\text{documents containing term}}})\]</span></p>
</div>
</div>
